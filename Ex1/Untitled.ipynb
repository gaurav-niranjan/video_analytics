{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "57daf72d-5973-4b0c-8eb3-c10b0814c02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from itertools import cycle\n",
    "from collections import deque\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9e1a6e2c-932b-4fd1-a052-8b593694a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP      = 5          # grid step (pixels)\n",
    "EIG_MULT = 1e-3       # constant in Eq.(1)\n",
    "TRACK_LEN   = 15         # L in the paper (Eq. 3)\n",
    "JUMP_FRAC   = 0.7        # drop if a single step ≥ 70 % of total path\n",
    "MIN_TOTAL   = 1.0        # drop static paths (< 1 px overall motion)\n",
    "\n",
    "FLOW_KW     = dict(pyr_scale=0.5, levels=3, winsize=15,\n",
    "                   iterations=3, poly_n=5, poly_sigma=1.2, flags=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "df7b6837-993d-40aa-ba05-2cd1a4fb7dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dense_points(gray: np.ndarray,\n",
    "                        step: int = 5,\n",
    "                        eig_mult: float = 1e-3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return an (N,2) int32 array of good seed points for one frame.\n",
    "    Implements Eq.(1): reject texture-less pixels.\n",
    "    \"\"\"\n",
    "    H, W = gray.shape\n",
    "    ys, xs = np.mgrid[0:H:step, 0:W:step]           # regular grid\n",
    "    grid   = np.vstack((xs.ravel(), ys.ravel())).T  # (N,2)\n",
    "\n",
    "    saliency = cv2.cornerMinEigenVal(gray, 3, 3)    # Shi–Tomasi λ_min\n",
    "    thresh   = eig_mult * saliency.max()\n",
    "    keep     = saliency[grid[:, 1], grid[:, 0]] >= thresh\n",
    "    return grid[keep].astype(np.float32)            # (x,y) float for sub-pixel\n",
    "\n",
    "def compute_flow(prev, curr):\n",
    "\n",
    "    return cv2.calcOpticalFlowFarneback(\n",
    "        prev, curr, None,\n",
    "        0.5,\n",
    "        3,\n",
    "        15,\n",
    "        3,\n",
    "        5, 1.2, 0\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60b5ef8e-3676-4741-b7ab-63d4826576ca",
   "metadata": {},
   "source": [
    "cap = cv2.VideoCapture('datasets/dummy.avi')\n",
    "\n",
    "ret, frame = cap.read()\n",
    "\n",
    "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "pts = sample_dense_points(gray)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d584ae54-f324-4762-b042-8fbfb1de84b2",
   "metadata": {},
   "source": [
    "# visual check\n",
    "vis = frame.copy()\n",
    "for x, y in pts.astype(int):\n",
    "    cv2.circle(vis, (x, y), 1, (0,255,0), -1)\n",
    "cv2.imshow(\"dense seeds\", vis); cv2.waitKey(0)\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8794a67a-8a22-4fb3-be7f-bb9334e78811",
   "metadata": {},
   "source": [
    "cap = cv2.VideoCapture('datasets/dummy.avi')\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Could not open video\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:                       # end‑of‑file\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    seeds = sample_dense_points(gray)        # (N,2) int array\n",
    "\n",
    "    # draw every seed as a tiny green dot\n",
    "    vis = frame.copy()\n",
    "    for x, y in seeds.astype(int):\n",
    "        cv2.circle(vis, (x, y), 1, (0,255,0), -1)\n",
    "\n",
    "    cv2.imshow(\"Dense seeds (press q to quit)\", vis)\n",
    "\n",
    "    # stop when the user hits ‘q’\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6d97f4ed-83dc-4bf6-975c-ebee2676b35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advance_tracks(tracks, flow,\n",
    "                   jump_frac=0.7,\n",
    "                   track_len=15,\n",
    "                   min_total=0.5):\n",
    "    \"\"\"\n",
    "    Move every active track one step forward.\n",
    "    Returns (new_active, finished_list)\n",
    "    finished_list items are length-track_len lists of (x,y).\n",
    "    \"\"\"\n",
    "    H, W, _ = flow.shape\n",
    "    new_active, finished = [], []\n",
    "\n",
    "    for tr in tracks:\n",
    "        #print(tr)\n",
    "        x, y = tr[-1]\n",
    "        dx, dy = flow[int(y), int(x)]\n",
    "        nx, ny = x + dx, y + dy\n",
    "\n",
    "        # out-of-frame?\n",
    "        if not (0 <= nx < W and 0 <= ny < H):\n",
    "            continue\n",
    "\n",
    "        # sanity-check\n",
    "        \n",
    "        total_disp = np.hypot(nx - tr[0][0], ny - tr[0][1])\n",
    "        #print(f'total_disp from the start of the trajectory:{total_disp}')\n",
    "        #print(f'Current displacement: {np.hypot(dx, dy)}')\n",
    "        if len(tr) > 1:\n",
    "            if np.hypot(dx, dy) > jump_frac * (total_disp + 1e-8):\n",
    "                #print('dropping trajectory due to high displacement.')\n",
    "                continue                              # drop erratic\n",
    "\n",
    "        tr.append((nx, ny))\n",
    "        if len(tr) == track_len:\n",
    "            if total_disp > min_total:\n",
    "                finished.append(tr)\n",
    "        else:\n",
    "            new_active.append(tr)\n",
    "\n",
    "    return new_active, finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b9abe262-cda6-4859-999c-1f4b51e6a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The paper wants one active point per STEP × STEP grid cell at all times.\n",
    "def reseed(tracks, gray, step):\n",
    "    \"\"\"\n",
    "    Fill every step×step cell not already occupied by a track head.\n",
    "    Modifies 'tracks' in-place by appending [seed] lists.\n",
    "    \"\"\"\n",
    "    seeds = sample_dense_points(gray, step)\n",
    "    if tracks:\n",
    "        heads = np.array([tr[-1] for tr in tracks])\n",
    "        dist  = np.linalg.norm(seeds[:, None] - heads[None], axis=2)\n",
    "        seeds = seeds[dist.min(axis=1) >= step / 2] #If the cell already has a head, you discard that seed — \n",
    "                 #it’s already covered.If no head is in that cell, you keep the seed and start a new track.\n",
    "        #print(seeds)\n",
    "    tracks.extend([tuple(p)] for p in seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8903073a-730b-4908-aca7-d81c0749b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trajectory_shape(track):\n",
    "    \"\"\"30-D normalised displacement vector (Eq. 3).\"\"\"\n",
    "    disp = np.diff(np.array(track), axis=0)          # (15,2)\n",
    "    norm = np.linalg.norm(disp, axis=1).sum() + 1e-8\n",
    "    return (disp / norm).flatten().astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "17aef1bd-b00e-4233-9c55-f94fa84c01b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dense_trajectories(video_path,\n",
    "                               step: int = 5,\n",
    "                               track_len: int = 16,\n",
    "                               return_tracks: bool = False):\n",
    "    \"\"\"\n",
    "    Track dense points and return trajectory-shape descriptors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    video_path   : str / Path\n",
    "    step         : grid spacing  (pixels)\n",
    "    track_len    : #positions per trajectory (16 → 15 displacements = 30-D)\n",
    "    return_tracks: if True → also return {frame_idx: [...] } dictionaries\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    If return_tracks == False:\n",
    "        desc30 : (N, 30) float32 array\n",
    "    If return_tracks == True:\n",
    "        finished_by_frame : dict[int, list[list[(x,y)]]]\n",
    "        shapes_by_frame   : dict[int, list[np.ndarray shape (30,)]]\n",
    "        desc30            : (N, 30) float32 array\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    ok, frame = cap.read()\n",
    "    if not ok:\n",
    "        raise IOError(f\"Cannot open {video_path}\")\n",
    "\n",
    "    prev_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # ── containers ──────────────────────────────────────────────────────\n",
    "    tracks        = []       # active, still-growing lists of (x,y)\n",
    "    descriptors   = []       # 30-D vectors (all finished)\n",
    "    finished_dict = {}       # key = end frame idx,   value = list of tracks\n",
    "    shape_dict    = {}       # key = end frame idx,   value = list of shape30\n",
    "\n",
    "    # ── seed frame 0 ───────────────────────────────────────────────────\n",
    "    tracks.extend([tuple(p)] for p in sample_dense_points(prev_gray, step))\n",
    "\n",
    "    frame_idx = 0            # keeps track of *prev* frame’s index\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # A. Farnebäck flow prev → curr\n",
    "        flow = compute_flow(prev_gray, gray)\n",
    "\n",
    "        # B. advance tracks, collect those that hit length = track_len\n",
    "        tracks, done = advance_tracks(tracks, flow, track_len=track_len)\n",
    "\n",
    "        for tr in done:\n",
    "            shape30 = trajectory_shape(tr)\n",
    "            descriptors.append(shape30)\n",
    "\n",
    "            if return_tracks:                       # store for Task-2\n",
    "                finished_dict.setdefault(frame_idx, []).append(tr)\n",
    "                shape_dict   .setdefault(frame_idx, []).append(shape30)\n",
    "\n",
    "        # C. reseed empty grid cells on current frame\n",
    "        reseed(tracks, gray, step)\n",
    "\n",
    "        # slide window\n",
    "        prev_gray = gray\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    desc30 = (np.vstack(descriptors)\n",
    "              if descriptors else np.empty((0, 30), np.float32))\n",
    "\n",
    "    if return_tracks:\n",
    "        return finished_dict, shape_dict, desc30\n",
    "    else:\n",
    "        return desc30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c4388745-b69e-4871-b4ee-89ee7febdf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_dict, shape_dict, desc30 = extract_dense_trajectories(\"datasets/dummy.avi\", return_tracks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "23fa406a-47e2-4204-82e3-a15dde75a249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146, 30)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc30.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d4f4b25a-68a4-4915-8e05-459011bdadc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finished_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2d056d94-c2dd-4f53-9495-40f9e097ef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick six bright colours and recycle them\n",
    "_colour_cycle = cycle([(255,0,0), (0,255,0), (0,0,255),\n",
    "                       (255,255,0), (255,0,255), (0,255,255)])\n",
    "\n",
    "def draw_done_paths(frame, done, colours=None, thickness=1):\n",
    "    \"\"\"\n",
    "    frame    : BGR image you want to paint on (will be *modified* in-place)\n",
    "    done     : list of finished trajectories; each is [(x0,y0), …, (x15,y15)]\n",
    "    colours  : optional iterable of BGR tuples; will be recycled\n",
    "    thickness: line width in pixels\n",
    "    \"\"\"\n",
    "    if colours is None:\n",
    "        colours = _colour_cycle\n",
    "    for tr, col in zip(done, colours):\n",
    "        for p1, p2 in zip(tr[:-1], tr[1:]):         # draw segment chain\n",
    "            cv2.line(frame,\n",
    "                     (int(p1[0]), int(p1[1])),\n",
    "                     (int(p2[0]), int(p2[1])),\n",
    "                     col, thickness, cv2.LINE_AA)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "80dfef51-7229-4b41-83c5-aa17e4eea5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(\"datasets/dummy.avi\")\n",
    "_, first = cap.read()\n",
    "canvas = first.copy()\n",
    "\n",
    "for tr in done:          # list you kept during extraction\n",
    "    draw_done_paths(canvas, [tr])     # wrap tr in list so API matches\n",
    "cv2.imwrite(\"all_finished_paths.png\", canvas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "9f32e922-b4cf-415b-b21e-ff6f0a7792f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH = 32\n",
    "HALF = PATCH//2\n",
    "\n",
    "def crop_volume(frames, flows_u, flows_v, traj):\n",
    "\n",
    "    L = 15\n",
    "    H, W = frames[0].shape\n",
    "\n",
    "    img_vol = np.zeros((L, PATCH, PATCH), np.float32)\n",
    "    u_vol   = np.zeros_like(img_vol)\n",
    "    v_vol   = np.zeros_like(img_vol)\n",
    "\n",
    "    #fill each time dim (15) with the 32x32 patch centered on the trajectory head\n",
    "    #anything that falls outside the image stays zero\n",
    "\n",
    "    for t, (x, y) in enumerate(traj):\n",
    "        x, y = int(round(x)), int(round(y)) #flow tracking gave us float, but to slice the arrays we need int\n",
    "\n",
    "        #Original full frame bounds of the patch\n",
    "        x0, x1 = x - HALF, x + HALF #left-right bounds\n",
    "        y0, y1 = y - HALF, y + HALF #top-bottom bounds\n",
    "\n",
    "        #Clip these bounds so they stay inside the image\n",
    "        xs0, xs1 = max(0, x0), min(W, x1)\n",
    "        ys0, ys1 = max(0, y0), min(H, y1)\n",
    "\n",
    "        #The patches might bet clipped to not be 32x32 anymore, if they out of image bounds\n",
    "        #Calulcate the offset for zero padding (the arrays we fill are already full of zeros)\n",
    "        #So we only fill in those places which have a value from the frame\n",
    "\n",
    "        dx0, dx1 = xs0-x0, PATCH - (x1 - xs1)\n",
    "        dy0, dy1 = ys0-y0, PATCH - (y1 - ys1)\n",
    "\n",
    "        img_vol[t, dy0:dy1, dx0:dx1] = frames[t][ys0:ys1, xs0:xs1]\n",
    "        u_vol  [t, dy0:dy1, dx0:dx1] = flows_u[t][ys0:ys1, xs0:xs1]\n",
    "        v_vol  [t, dy0:dy1, dx0:dx1] = flows_v[t][ys0:ys1, xs0:xs1]\n",
    "\n",
    "        return img_vol, u_vol, v_vol\n",
    "        \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "d64e35f4-20c1-4d44-9ecd-31cbd967175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tube_generate(vol):\n",
    "\n",
    "    for ti in range(3):\n",
    "        t0, t1 = ti*5, (ti+1)*5\n",
    "        for ry in range(2):\n",
    "            r0, r1 = ry*16, (ry+1)*16\n",
    "            for cx in range(2):\n",
    "                c0, c1 = cx*16, (cx+1)*16\n",
    "                yield (t0, t1, r0, r1, c0, c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e1de4a46-3190-4df2-9d6e-e789fb1d2a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed bin edges\n",
    "BIN_HOG = 8\n",
    "BIN_HOF = 9          # 8 orient + 1 static\n",
    "BIN_MBH = 8\n",
    "\n",
    "def hog_hist(patch):\n",
    "    gx = cv2.Sobel(patch, cv2.CV_32F, 1, 0, ksize=1)\n",
    "    gy = cv2.Sobel(patch, cv2.CV_32F, 0, 1, ksize=1)\n",
    "    mag, ang = cv2.cartToPolar(gx, gy, angleInDegrees=True)\n",
    "    bins = np.int32(ang // (360/BIN_HOG)) % BIN_HOG\n",
    "    hist = np.bincount(bins.ravel(), mag.ravel(), BIN_HOG).astype(np.float32)\n",
    "    return hist / (np.linalg.norm(hist)+1e-6)\n",
    "\n",
    "def hof_hist(u_patch, v_patch, static_thr=1.0):\n",
    "    mag, ang = cv2.cartToPolar(u_patch, v_patch, angleInDegrees=True)\n",
    "    static = mag < static_thr\n",
    "    bins = np.int32(ang // (360/8)) % 8\n",
    "    bins[static] = 8                      # 9th bin\n",
    "    hist = np.bincount(bins.ravel(), mag.ravel(), BIN_HOF).astype(np.float32)\n",
    "    return hist / (np.linalg.norm(hist)+1e-6)\n",
    "\n",
    "def mbh_hist(comp_patch):\n",
    "    # comp_patch is u or v flow channel\n",
    "    gx = cv2.Sobel(comp_patch, cv2.CV_32F, 1, 0, ksize=1)\n",
    "    gy = cv2.Sobel(comp_patch, cv2.CV_32F, 0, 1, ksize=1)\n",
    "    mag, ang = cv2.cartToPolar(gx, gy, angleInDegrees=True)\n",
    "    bins = np.int32(ang // (360/BIN_MBH)) % BIN_MBH\n",
    "    hist = np.bincount(bins.ravel(), mag.ravel(), BIN_MBH).astype(np.float32)\n",
    "    return hist / (np.linalg.norm(hist)+1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "84528e45-fecd-4c28-9ecb-ef6d0f0efb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tube_descriptors(img_vol, u_vol, v_vol):\n",
    "    HOG, HOF, MBHx, MBHy = [], [], [], []\n",
    "\n",
    "    for t0,t1,r0,r1,c0,c1 in tube_generate(img_vol):\n",
    "        # slice each 16×16×5 tube\n",
    "        img_patch = img_vol[t0:t1, r0:r1, c0:c1]\n",
    "        u_patch   = u_vol  [t0:t1, r0:r1, c0:c1]\n",
    "        v_patch   = v_vol  [t0:t1, r0:r1, c0:c1]\n",
    "\n",
    "        HOG .append(hog_hist(img_patch))\n",
    "        HOF .append(hof_hist(u_patch, v_patch))\n",
    "        MBHx.append(mbh_hist(u_patch))\n",
    "        MBHy.append(mbh_hist(v_patch))\n",
    "\n",
    "    return (np.hstack(HOG),  np.hstack(HOF),\n",
    "            np.hstack(MBHx), np.hstack(MBHy))      # 96,108,96,96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a9e2f462-d711-4c62-9773-14e3fac48efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptor_426(frames, flows_u, flows_v, traj, shape30):\n",
    "    img_vol, u_vol, v_vol = crop_volume(frames, flows_u, flows_v, traj)\n",
    "    h_hog, h_hof, h_mbx, h_mby = tube_descriptors(img_vol, u_vol, v_vol)\n",
    "    return np.hstack((shape30, h_hog, h_hof, h_mbx, h_mby))   # 30+96+108+96+96"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f713e42-b0c8-4d3d-8eaa-7da412c33b8a",
   "metadata": {},
   "source": [
    "# Task 2: Compute histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "41098112-c0be-4c3d-a429-85c1a652411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_desc426(video):\n",
    "\n",
    "    finished_dict, shape_dict, desc30 = extract_dense_trajectories(video, return_tracks=True)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video)\n",
    "    ok, prev = cap.read()\n",
    "    prev_gray = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)\n",
    "    frames_buf, flows_u_buf, flows_v_buf = deque(maxlen=TRACK_LEN), deque(maxlen=TRACK_LEN), deque(maxlen=TRACK_LEN)\n",
    "\n",
    "    desc426 = []\n",
    "    frame_idx = 0\n",
    "    while ok:\n",
    "        gray = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)\n",
    "        frames_buf.append(gray)\n",
    "    \n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "        flow = cv2.calcOpticalFlowFarneback(gray, cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY),\n",
    "                                            None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "        flows_u_buf.append(flow[...,0]); flows_v_buf.append(flow[...,1])\n",
    "    \n",
    "        # when a trajectory ends on *this* frame, build its 426-D descriptor\n",
    "        for tr, shape30 in zip(finished_dict.get(frame_idx, []), shape_dict.get(frame_idx, [])):\n",
    "            if len(frames_buf) == 15:  # buffers full\n",
    "                d426 = descriptor_426(list(frames_buf),\n",
    "                                      list(flows_u_buf),\n",
    "                                      list(flows_v_buf),\n",
    "                                      tr, shape30)\n",
    "                desc426.append(d426)\n",
    "    \n",
    "        prev = frame\n",
    "        frame_idx += 1\n",
    "    \n",
    "    cap.release()\n",
    "    desc426 = np.array(desc426, np.float32)\n",
    "\n",
    "    return desc426"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b819e9-ed45-4149-9e78-0aa845ccd1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaa992bd-7c7c-4f02-aef0-1dd3b42f5baa",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "280e4dfc-cf57-41c3-8432-3e25251cb5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pca(desc426_stack, n_comp=64):\n",
    "    pca = PCA(n_components=n_comp, svd_solver='randomized', whiten=True)\n",
    "    pca.fit(desc426_stack)              # train on all 426-D descriptors\n",
    "    return pca\n",
    "\n",
    "def project_pca(pca, desc426):\n",
    "    return pca.transform(desc426).astype(np.float32)   # (N, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "0403f436-c524-4b97-ac97-b3cec4d252da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gmm(desc64_stack, n_comp=4):\n",
    "    gmm = GaussianMixture(n_components=n_comp,\n",
    "                          covariance_type='diag',\n",
    "                          max_iter=100, verbose=1)\n",
    "    gmm.fit(desc64_stack)\n",
    "    return gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "e3a2c3fa-a60d-462b-bb65-5efc8c3b8aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_vector(desc64, gmm):\n",
    "    \"\"\"\n",
    "    desc64 : (N_i , 64) array of one video\n",
    "    Returns : (2*K*64,) FV with signed square-root + L2 norm\n",
    "    \"\"\"\n",
    "    Q = gmm.predict_proba(desc64)          # (N_i, K) posteriors\n",
    "    N = desc64.shape[0]\n",
    "\n",
    "    # Compute first-order (means)\n",
    "    diff = desc64[:, None, :] - gmm.means_  # (N_i,K,64)\n",
    "    sigma = np.sqrt(gmm.covariances_)       # diag σ\n",
    "    diff /= sigma                           # normalize\n",
    "    S1 = (Q[..., None] * diff).sum(axis=0) / np.sqrt(gmm.weights_)[:, None]\n",
    "\n",
    "    # Second-order (variances)\n",
    "    diff2 = (diff**2 - 1)\n",
    "    S2 = (Q[..., None] * diff2).sum(axis=0) / np.sqrt(2*gmm.weights_)[:, None]\n",
    "\n",
    "    fv = np.hstack((S1.flatten(), S2.flatten())).astype(np.float32)\n",
    "\n",
    "    # power- & L2-norm\n",
    "    fv = np.sign(fv) * np.sqrt(np.abs(fv))\n",
    "    fv /= (np.linalg.norm(fv) + 1e-8)\n",
    "    return fv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "20c23671-1dae-4c8a-ab89-29e133fff2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_fv(descriptors426, pca, gmm):\n",
    "    desc64 = project_pca(pca, descriptors426)   # (N,64)\n",
    "    return fisher_vector(desc64, gmm)           # (2*K*64,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "a989cb04-aec3-4c2a-80e6-220e162953bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(X_train, y_train, C=1.0):\n",
    "    clf = LinearSVC(C=C)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf\n",
    "\n",
    "def evaluate(clf, X_test, y_test):\n",
    "    pred = clf.predict(X_test)\n",
    "    acc  = accuracy_score(y_test, pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce7d99e-9db1-4eee-a2ac-1adf0da82d52",
   "metadata": {},
   "source": [
    "# Task 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "24a697c3-413f-4c18-a093-9987ee97e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_FV = Path(\"fv_cache\")      # one .npy per video\n",
    "CACHE_FV.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "808fe47b-7299-4727-a190-ac3559a1edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_split(txt):\n",
    "    vids, labels = [], []\n",
    "    for line in Path(txt).read_text().splitlines():\n",
    "        fname, lab = line.strip().split()\n",
    "        vids.append(os.path.join(os.getcwd(),f'datasets/UCF-3/{fname}'))\n",
    "        labels.append(int(lab))\n",
    "    return vids, np.array(labels, np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "ef05d941-e8e2-47d1-a8e3-785c9589ca86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train / test file lists …\n",
      "/Users/gauravniranjan/Documents/Video Analytics/Assignments/Ex1/datasets/UCF-3\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading train / test file lists …\")\n",
    "dataset_dir = os.path.join(os.getcwd(), 'datasets/UCF-3')\n",
    "print(dataset_dir)\n",
    "train_vids, y_train = read_split(os.path.join(dataset_dir, 'train.txt'))\n",
    "test_vids,  y_test  = read_split(os.path.join(dataset_dir, 'test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "708969ce-da69-4947-a070-a7be7a1a4579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering 426-D descriptors to fit PCA …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 105/105 [1:12:42<00:00, 41.54s/it]\n"
     ]
    }
   ],
   "source": [
    "print(\"Gathering 426-D descriptors to fit PCA …\")\n",
    "all_desc = []\n",
    "for vid in tqdm(train_vids):\n",
    "    desc426 = compute_desc426(vid)         # Task-2 output\n",
    "    all_desc.append(desc426)\n",
    "all_desc = np.vstack(all_desc)                 # shape (M, 426)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "105f9e0f-6d5f-475a-a74d-5a6b936579c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 64-D PCA on 1386449 train descriptors …\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting 64-D PCA on\", all_desc.shape[0], \"train descriptors …\")\n",
    "pca = fit_pca(all_desc, n_comp=64)\n",
    "desc64 = pca.transform(all_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "bec712c1-1dcc-4440-95c6-534eefe1317d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMM …\n",
      "Initialization 0\n",
      "  Iteration 10\n",
      "  Iteration 20\n",
      "Initialization converged.\n"
     ]
    }
   ],
   "source": [
    "print(\"GMM …\")\n",
    "gmm = fit_gmm(desc64, n_comp=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "3445fd8b-76e2-4d2e-ab32-de20a2a15289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fv_for_video(vid):\n",
    "    fv_file = CACHE_FV / (Path(vid).stem + \".npy\")\n",
    "    if fv_file.exists():\n",
    "        return np.load(fv_file)\n",
    "    # else compute from scratch\n",
    "    desc426 = compute_desc426(vid)\n",
    "    fv = video_fv(desc426, pca, gmm)           # returns (FVdim,)\n",
    "    np.save(fv_file, fv)\n",
    "    return fv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e90d18-dc45-43bb-a7f8-03601a381ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building Fisher vectors …\")\n",
    "X_train = np.vstack([fv_for_video(v) for v in tqdm(train_vids)])\n",
    "X_test  = np.vstack([fv_for_video(v) for v in tqdm(test_vids)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "ab45df66-7289-4286-82c7-ca25dd9f2f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature dims: (105, 640) (45, 640)\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature dims:\", X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "9dfea004-dc23-4208-ac9a-ec532ea4ff5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC(dual=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LinearSVC</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.svm.LinearSVC.html\">?<span>Documentation for LinearSVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LinearSVC(dual=False)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC(dual=False)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearSVC(C=1.0, dual=False)             # dual=False for high-dim data\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "a25a686a-94e0-4479-8d14-61dd2d78876f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 93.33%\n",
      "Confusion matrix:\n",
      " [[15  0  0]\n",
      " [ 0 12  3]\n",
      " [ 0  0 15]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d5be82-7ca5-446a-a84a-a094a297973b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
